{"cells":[{"cell_type":"markdown","metadata":{"id":"Oe2js4BMPc9W"},"source":["### What resource we need to Train this large data??: GPU"]},{"cell_type":"markdown","metadata":{"id":"1Z6Vn4bqNvLA"},"source":["### Completed Tasks:\n","\n","--> GEE Asset/Crop polygons\n","\n","--> polygon area estimation for each crop type data\n","\n","--> Plante/NICFI Pixel count for each crops, each polygons\n","\n","--> Graphic representation for area vs Pixel count"]},{"cell_type":"markdown","metadata":{"id":"o0EPLwkbNSX_"},"source":["### Next tasks:\n","\n","--> Cloud masking for Planet (Algorthms)\n","\n","--> Image Normalization: Scale the pixel values (e.g., 0-255) to a range that is typical for neural network inputs, such as 0-1 or -1 to 1.\n","\n","--> Data Augmentation: To increase the diversity of the training data and prevent overfitting, apply transformations like rotations, translations, scaling, and horizontal flipping.\n","\n","--> Patch Extraction: For high-resolution images, it might be necessary to create smaller, manageable patches. This makes the training process more efficient and helps in handling large images during deployment.\n","\n","--> DL (FCNN) Trainings\n","--> Parallel ML training"]},{"cell_type":"markdown","metadata":{"id":"6uHyL3LNf6f7"},"source":["### Reading corner about crop mapping in Senegal\n","SEN4STAT/ESA: https://www.esa-sen4stat.org/user-stories/senegal-prototype/\n","\n","EOSTAT/FAO: https://data.apps.fao.org/catalog/dataset/5c377b2b-3c2e-4b70-afd7-0c80900b68bb/resource/50bc9ff5-95d2-40cd-af12-6aee2cfcc4ae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aaqStUK6zab"},"outputs":[],"source":["#Lib imports:\n","import ee\n","#print('Using EE version ', ee.__version__)\n","import folium\n","#print('Using Folium version ', folium.__version__)\n","from os import MFD_HUGE_1MB\n","import pandas as pd\n","import time\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#import cv2\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","#print('Using TF version ', tf.__version__)\n","from typing import Dict, Iterable, List, Tuple\n","#from google.colab import auth\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcKR75Me7D6G"},"outputs":[],"source":["# authenticate with user credentials\n","#auth.authenticate_user()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKVguRub1wd_"},"outputs":[],"source":["# Authenticate to the Earth Engine servers\n","ee.Authenticate()\n","\n","# Initialize the Earth Engine object with Google Cloud project ID\n","project_id = 'ee-kkidia3'\n","ee.Initialize(project=project_id)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"NHmwEttlt5TV"},"outputs":[],"source":["# @title Default title text\n","\n","def list_ee_assets(folder_path):\n","    \"\"\" List all assets in the specified Earth Engine folder path \"\"\"\n","    try:\n","        assets_list = ee.data.getList({'id': folder_path})\n","        for asset in assets_list:\n","            print(asset['id'])\n","    except Exception as e:\n","        print(\"Error accessing Earth Engine assets:\", e)\n","\n","# GEE Asset path\n","#folder_path = 'users/kkidia3'\n","\n","# Call the function to list assets\n","#list_ee_assets(folder_path)\n"]},{"cell_type":"markdown","metadata":{"id":"onPAzxk-kipr"},"source":["### Crop fields polygon for rainfed season 2018-2020"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XkdpVMek6P4"},"outputs":[],"source":["#merge for the rainfed croping season 2018-2020.\n","merge18_20 = data_2018.merge(data_2019).merge(data_2020)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVDH9JieBgYJ"},"outputs":[],"source":["# Import necessary librari\n","from google.colab import drive\n","\n","# Function to calculate NDVI\n","def calculate_ndvi(image):\n","    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n","    return image.addBands(ndvi)\n","\n","# Load Sentinel-2 imagery and filter by date and bounds of the polygons\n","sentinel2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n","    .filterBounds(data_2020).filterDate('2020-09-01', '2020-09-28') \\\n","    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\\\n","    .map(calculate_ndvi)\n","\n","# Select bands to use for classification\n","bands = ['B2', 'B3', 'B4', 'B8', 'NDVI']\n","\n","# Reduce the image collection to median to get a single composite image\n","image = sentinel2.median().select(bands)\n","\n","# Function to sample data from the image and integrate with feature collection\n","def add_bands_to_fc(fc, image):\n","    def add_bands(feature):\n","        sampled = image.reduceRegion(           #Reduce the image collection to get a median composite image.\n","            reducer=ee.Reducer.mean(),\n","            geometry=feature.geometry(),\n","            scale=10,\n","            maxPixels=1e13\n","        )\n","        return feature.set(sampled)   #Create a function to add bands to the feature collection by sampling the image.\n","    return fc.map(add_bands)\n","\n","# Integrate the bands into the feature collection\n","data_2020_with_bands = add_bands_to_fc(data_2020, image)  # using teh cleaned data -- can change to the original data\n","\n","#Convert FeatureCollection to pandas DataFrame\n","def fc_to_df(fc):\n","    features = fc.getInfo()['features']\n","    dict_list = [f['properties'] for f in features]\n","    df = pd.DataFrame(dict_list)\n","    return df\n","\n","# Display the first 5 rows of the dataframe\n","df = fc_to_df(data_2020_with_bands)\n","print(df.head())\n","\n","# Export the new shapefile dataset to Google Drive\n","export_task = ee.batch.Export.table.toDrive(\n","    collection=data_2020_with_bands,\n","    description='data_2020_with_bands_sep',\n","    fileFormat='SHP'\n",")\n","\n","export_task.start()\n","\n","# Monitor the task status\n","while export_task.active():\n","    print('Polling for task (id: {}).'.format(export_task.id))\n","    time.sleep(30)\n","\n","print('Export task completed with status:', export_task.status())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DwlUQdrjQzI"},"outputs":[],"source":["# Function to get feature properties\n","def get_properties(feature):\n","    return ee.Feature(None, feature.toDictionary())\n","\n","# Map the function over the collection\n","properties_list = data_2018.map(get_properties).getInfo()\n","\n","# Extract properties from the list of features\n","properties_list = [feature['properties'] for feature in properties_list['features']]\n","\n","# Convert the list of dictionaries to a DataFrame\n","df = pd.DataFrame(properties_list)\n","\n","# Rename columns if needed\n","#df.rename(columns={'Annee': 'Year'}, inplace=True) #df.rename(columns={'old_column_name': 'new_column_name'}, inplace=True)\n","\n","\n","# Display the DataFrame\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwPMJy6wnhsi"},"outputs":[],"source":["# @title Crop_Ncrop\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","df.groupby('Crop_Ncrop').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrCAC6fmm86c"},"outputs":[],"source":["# @title Average Area by Crop Type\n","\n","df.groupby('Speculatio')['Sup_ha'].mean().plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZhvsQ1OmZgz"},"outputs":[],"source":["# @title Admin1\n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","df.groupby('Admin1').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp9dkcLqdtnM"},"outputs":[],"source":["# Check for NaN values in each column\n","nan_counts = df.isna().sum()\n","\n","# Display the columns with their respective NaN counts\n","print(nan_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnaVCgZdd3tg"},"outputs":[],"source":["# Display only columns with NaN values\n","nan_columns = nan_counts[nan_counts > 0]\n","print(nan_columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDtRO5zDlYo9"},"outputs":[],"source":["# @title Default title text\n","# Define a function to add Earth Engine vector data as a layer to a Folium map\n","def add_ee_vector_layer(feature_collection, style, layer_name, map_object):\n","    painted = ee.Image().paint(feature_collection, 'constant', 2)  # Here 'constant' is a dummy property for visualization\n","    map_id_dict = painted.getMapId(style)\n","    folium.TileLayer(\n","        tiles=map_id_dict['tile_fetcher'].url_format,\n","        attr='Map Data &copy; Google Earth Engine',\n","        name=layer_name,\n","        overlay=True,\n","        control=True\n","    ).add_to(map_object)\n","\n","# Create a Folium map object\n","center = [14.4974, -14.4524]  # Center of the map (e.g., Senegal)\n","m1 = folium.Map(location=center, zoom_start=7)\n","\n","# Styling for the vector layer\n","style = {\n","    'color': 'blue',  # Line color\n","    'fillColor': '00000000',  # Fill color with opacity (00)\n","}\n","\n","# Add the merged crop fields to the map\n","add_ee_vector_layer(data_2018, style, 'Merged Crops 2019-2020', m1)\n","\n","# Add a layer control panel to the map\n","folium.LayerControl().add_to(m1)\n","\n","# Display the map\n","m1\n"]},{"cell_type":"markdown","metadata":{"id":"tVWi-cw_kcn7"},"source":["### Polygons for FY2023"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6VLDp2rv8JE"},"outputs":[],"source":["# Function to load individual crop feature collections\n","def load_crop_data(asset_id):\n","    return ee.FeatureCollection(asset_id)\n","\n","\n","# Merge all crop fields into a single feature collection except fallow as not part of crop field\n","merged2023 = gnut.merge(gsorrel).merge(cashew).merge(cassava).merge(cowpea)\\\n","                .merge(eggplant).merge(fonio).merge(gsorrel)\\\n","                .merge(maize).merge(millet).merge(okra).merge(potato)\\\n","                .merge(rice).merge(sesame).merge(sorgh).merge(soye).merge(squash).merge(taro)\\\n","                .merge(vouandzou).merge(melon).merge(wheat).merge(milletmix).merge(gnutmix).merge(cowpmix)#.merge(fallow)\n","\n","fallow = fallow.filter(ee.Filter.eq('class', 'Fallow')) #.merge(fallow) is not a crop\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDb9ZbKaNjuv"},"outputs":[],"source":["print(merged2023.size().getInfo())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhTw6LAYN1Le"},"outputs":[],"source":["print(fallow.size().getInfo())"]},{"cell_type":"markdown","metadata":{"id":"UEhnPNgHfEgp"},"source":["Next steps:\n","- create shape file with bands and NDVI\n","- edit the clumen class of crop, fallow, N-crop etc\n","- export the classes and bands in to Google earht engine asset\n","- and test the new asset if band values and added class are in GEE ASSET"]},{"cell_type":"markdown","metadata":{"id":"gVEDXrS3SAln"},"source":["### Normalized bands : integrate the bands in to shapfle (Vector) ðŸ“‚\n","\n","To normalize the spectral bands, we'll create a function that normalizes each band by subtracting the minimum value and dividing by the range (max - min) of that band. We'll apply this function to each band in the image collection before calculating the NDVI and proceeding with the rest of the steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_uQwnJeSAMA"},"outputs":[],"source":["# Function to normalize a band\n","def normalize_band(image, band_name, geometry):\n","    band = image.select(band_name)\n","    min_val = ee.Number(band.reduceRegion(ee.Reducer.min(), geometry, scale=10).get(band_name))\n","    max_val = ee.Number(band.reduceRegion(ee.Reducer.max(), geometry, scale=10).get(band_name))\n","    normalized_band = band.subtract(min_val).divide(max_val.subtract(min_val)).float()\n","    return image.addBands(normalized_band.rename(band_name + '_norm'), overwrite=True)\n","\n","# Function to normalize all specified bands\n","def normalize_bands(image, band_names, geometry):\n","    for band_name in band_names:\n","        image = normalize_band(image, band_name, geometry)\n","    return image\n","\n","# Function to calculate NDVI\n","def calculate_ndvi(image):\n","    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI') #(['B8_norm', 'B4_norm']).rename('NDVI')?????\n","    return image.addBands(ndvi)\n","\n","# Load Sentinel-2 imagery and filter by date and bounds of the polygons\n","sentinel2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n","    .filterBounds(gnut) \\\n","    .filterDate('2023-09-01', '2023-09-30') \\\n","    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \\\n","    .map(lambda image: normalize_bands(image, ['B2', 'B3', 'B4', 'B8'], gnut.geometry())) \\\n","    .map(calculate_ndvi)\n","\n","# Select bands to use for classification\n","bands = ['B2_norm', 'B3_norm', 'B4_norm', 'B8_norm', 'NDVI', 'B2', 'B3','B4', 'B8'] #'NDVI_norm\n","\n","# Reduce the image collection to median to get a single composite image\n","image = sentinel2.median().select(bands)\n","\n","# Function to sample data from the image and integrate with feature collection\n","def add_bands_to_fc(fc, image):\n","    def add_bands(feature):\n","        sampled = image.reduceRegion(\n","            reducer=ee.Reducer.mean(),\n","            geometry=feature.geometry(),\n","            scale=10,\n","            maxPixels=1e13,\n","            bestEffort=True\n","        )\n","        # Replace any null values with 0\n","        sampled = ee.Dictionary(sampled).map(lambda k, v: ee.Algorithms.If(v, v, ee.Number(0))) #????\n","        return feature.set(sampled)\n","    return fc.map(add_bands)\n","\n","# Integrate the bands into the feature collection\n","crop_with_bands = add_bands_to_fc(gnut, image)\n","\n","# Add new properties to each feature # edit the column or classes\n","\n","def add_properties(feature):\n","    return feature.set('class', 'crop').set('sub_class', 'legume').set('year', '2023').set('region', '???')\n","\n","merged2023_class_with_properties = crop_with_bands.map(add_properties)\n","\n","# Get the first few features to display their properties\n","features_to_display = merged2023_class_with_properties.limit(5).getInfo()['features']\n","\n","# Extract properties into a list of dictionaries\n","properties_list = [feature['properties'] for feature in features_to_display]\n","\n","# Create a DataFrame\n","\n","\n","# Create a DataFrame from the properties list\n","df = pd.DataFrame(properties_list)\n","\n","# Display the DataFrame\n","print(df.tail()) #print(df.head())\n","\n","# Export the modified Feature Collection to Google Earth Engine Asset\n","export_task = ee.batch.Export.table.toAsset(\n","    collection=merged2023_class_with_properties,\n","    description='Export Crop_normalized_2023 with classes',\n","    assetId='projects/ee-kkidia3/assets/groundnut_class_check9' #i will change based on Crop_2023_norm_bands_class\n",")\n","\n","export_task.start()\n","\n","print(\"Export task started. Check the Earth Engine Code Editor for task status.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBa_7hXJnM1P"},"outputs":[],"source":["# Export the modified Feature Collection to Google Drive as a shapefile\n","export_task = ee.batch.Export.table.toDrive(\n","    collection=merged2023_class_with_properties,\n","    description='Export Crop_normalized_2023 with classes',\n","    fileFormat='SHP',\n","    folder='crop',\n","    fileNamePrefix='groundnut_class_check5'  # Change based on Crop_2023_norm_bands_class\n",")\n","\n","export_task.start()"]},{"cell_type":"markdown","metadata":{"id":"cTGQwXs2VNZ4"},"source":["### Edit column of the polygon and rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGDFDj8XOtnY"},"outputs":[],"source":["# gnut_class = gnut_2023_with_bands\n","# # Function to add the new properties to each feature\n","# def add_properties(feature):\n","#     return feature.set('class', 'crop').set('year', '2023').set('region', '???')#.set('groundnut', 'groundnut') # change any column\n","\n","# # Map the function over the Feature Collection\n","# gnut_class_with_properties = gnut_class.map(add_properties)\n","\n","# # Get the first few features to display their properties\n","# features_to_display =gnut_class_with_properties.limit(5).getInfo()['features']\n","\n","# # Extract properties into a list of dictionaries\n","# properties_list = [feature['properties'] for feature in features_to_display]\n","# # Print the properties of the first few features\n","# #for feature in features_to_display:\n","#    # print(feature['properties'])\n","\n","# # Create a DataFrame from the properties list\n","# df = pd.DataFrame(properties_list)\n","\n","\n","\n","# # Remove the column\n","# #df = df.drop(columns=['year'], errors='ignore') #remove a column e.g ['timestamp']\n","\n","\n","# # Adding a new row\n","# #new_row = {'class': 'crop', 'groundnut': 'new_groundnut', 'other_column': 'value'}\n","# #df = df.append(new_row, ignore_index=True)\n","\n","# # Remove a row by index (e.g., removing the first row)\n","# #df = df.drop(index=[0]) #0 = the first row\n","\n","# # Display the DataFrame\n","# #print(df)\n","\n","# df.head() #show it in data fram\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vqzLY_CSURnF"},"source":["### Export the modified polygon to GEE ASSET"]},{"cell_type":"markdown","metadata":{"id":"zHGyF6gmj6fk"},"source":["Test the exported GEE asset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f2Pp1Jtj4Wy"},"outputs":[],"source":["#test\n","xx = load_crop_data('projects/ee-kkidia3/assets/groundnut_class_check5')\n","# Get the first few features to display their properties\n","f_display =xx.limit(5).getInfo()['features']\n","\n","# Crop_2023_norm_bands_class = load_crop_data('projects/ee-kkidia3/assets/Crop_2023_No_norm_bands_class')\n","# # Get the first few features to display their properties\n","# f_display =Crop_2023_norm_bands_class.limit(5).getInfo()['features']\n","\n","# Extract properties into a list of dictionaries\n","p_list = [feature['properties'] for feature in f_display]\n","# Print the properties of the first few features\n","#for feature in features_to_display:\n","   # print(feature['properties'])\n","\n","# Create a DataFrame from the properties list\n","df = pd.DataFrame(p_list)\n","\n","df.head() #show it in data fram"]},{"cell_type":"markdown","metadata":{"id":"Wtu_Up1sVv3r"},"source":["Sample randomforest calssification only gnut"]},{"cell_type":"markdown","metadata":{"id":"VVshRcNt4XOY"},"source":["### Visulaize the polygons in map"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WYyHLwnX4VrV"},"outputs":[],"source":["# @title Default title text\n","# Define a function to add Earth Engine vector data as a layer to a Folium map\n","def add_ee_vector_layer(feature_collection, style, layer_name, map_object):\n","    painted = ee.Image().paint(feature_collection, 'constant', 2)  # Here 'constant' is a dummy property for visualization\n","    map_id_dict = painted.getMapId(style)\n","    folium.TileLayer(\n","        tiles=map_id_dict['tile_fetcher'].url_format,\n","        attr='Map Data &copy; Google Earth Engine',\n","        name=layer_name,\n","        overlay=True,\n","        control=True\n","    ).add_to(map_object)\n","\n","# Create a Folium map object\n","center = [14.4974, -14.4524]  # Center of the map in  Senegal\n","m = folium.Map(location=center, zoom_start=7)\n","\n","# Styling for the vector layer\n","style = {\n","    'color': 'blue',  # Line color\n","    'fillColor': '00000000',  # Fill color with opacity (00)\n","}\n","\n","# Add the merged crop fields to the map\n","add_ee_vector_layer(merged2023, style, 'Merged Crops 2023', m)\n","\n","# Add a layer control panel to the map\n","folium.LayerControl().add_to(m)\n","\n","# Display the map\n","#m"]},{"cell_type":"markdown","metadata":{"id":"BbkKpj3Gjakc"},"source":["Remove overlapping polygons from your merged feature collection. Overlaps is due to data collection"]},{"cell_type":"markdown","metadata":{"id":"2cyMHrZPxdQr"},"source":["For example let me sort out only two polygons out of maney gnut polygons for closer look and simple analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"-UXDK9RywbX3"},"outputs":[],"source":["# @title Default title text\n","# Select two specific polygons by their indices or properties. e.g gnut\n","# Here, I want to select the first two polygons.\n","poly_2 = gnut.toList(2)\n","\n","#print(poly_2)\n","\n","# Get the individual polygons\n","polygon1 = ee.Feature(poly_2.get(0))\n","polygon2 = ee.Feature(poly_2.get(1))\n","\n","# Print the geometries of the selected polygons to verify.\n","print('Polygon 1_Blue:', polygon1.geometry().getInfo())\n","print('Polygon 2_Red:', polygon2.geometry().getInfo())\n","\n","\n","# Create a map centered at an arbitrary point @polygon1\n","map_center = [13.237198228125724,-14.715474917616827]  #set this to a blue selected @polygon1\n","m = folium.Map(location=map_center, zoom_start=30) #\n","\n","# Define a function to add a feature to the folium map.\n","def add_feature_to_map(feature, map_obj, color):\n","    geom = feature.geometry().getInfo()\n","    coords = geom['coordinates']\n","    if geom['type'] == 'Polygon':\n","        folium.Polygon(locations=[(pt[1], pt[0]) for pt in coords[0]], color=color).add_to(map_obj)\n","    elif geom['type'] == 'MultiPolygon':\n","        for poly in coords:\n","            folium.Polygon(locations=[(pt[1], pt[0]) for pt in poly[0]], color=color).add_to(map_obj)\n","\n","# Add polygons to the map\n","add_feature_to_map(polygon1, m, 'blue')\n","add_feature_to_map(polygon2, m, 'red')\n","\n","# Display the map\n","m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RRCWAtzMgXZ"},"outputs":[],"source":["# Calculate the total area of polygons in square meters for Rainfed season 2023. Example groundnut\n","area = gnut.geometry().area().format('%.1f').getInfo()\n","print(f\"Area M2: {area} square meters OR,\")\n","\n","# Calculate the area in hectares (1 hectare = 10,000 square meters)\n","area_ha = gnut.geometry().area().divide(10000).format('%.1f').getInfo() # Convert square meters to hectares\n","print(f\"Area Ha: {area_ha} hectares\")\n","\n","# Calculate the perimeter in meters\n","perimeter = gnut.geometry().perimeter().format('%.1f').getInfo()\n","print(f\"Perimeter: {perimeter} meters\")\n","\n","# Get the centroid of the polygon\n","centroid = gnut.geometry().centroid().getInfo()\n","print(f\"Centroid: {centroid}\")\n","\n","# Get the bounding box as a GeoJSON\n","bounding_box = gnut.geometry().bounds().getInfo()\n","print(f\"Bounding Box: {bounding_box}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rn2qJaFPJhVb"},"source":["### Estimate the each polygon area (m2), for each crop e.g Groundnt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ4Dh3yNIChw"},"outputs":[],"source":["# Define a function to calculate the area of each polygon in hectares\n","#def calc_area(feature):\n","    #return feature.set('area_ha', ee.Number(feature.geometry().area().divide(10000)).format('%.3f'))\n","\n","# Define a function to calculate the area of each polygon in m2\n","def calc_area(feature):\n","  return feature.set('area_m2', ee.Number(feature.geometry().area())\n","  .format('%.1f'))#.divide(10000)) & # .format('%.1f') use to limit the digits\n","\n","# Apply the function to each feature in the collection\n","area_mapped = merged2023.map(calc_area)\n","\n","# Fetch the mapped area information from the server\n","areas_info = area_mapped.limit(10).getInfo()  # Limiting to the first 10 features directly to show everthing remove .limit(10)\n","\n","# Iterate through the first 10 features and print area in hectares with 3 decimal places\n","for feature in areas_info['features']:\n","    area = feature['properties']['area_m2'] ##m2\n","    print(f\"Gnut Polygon_Area: {area} m2\") #m2\n"]},{"cell_type":"markdown","metadata":{"id":"hDwoYgNvPUjs"},"source":["### Area (ha) distributions e.g Groundnut"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bj9r9SdyNQOL"},"outputs":[],"source":["# Define a function to calculate the area of each polygon in hectares\n","#def calc_area(feature):\n","    #return feature.set('area_ha', feature.geometry().area().divide(10000))  # Convert from square meters to hectares\n","\n","# Define a function to calculate the area of each polygon in hectares\n","def calc_area(feature):\n","    return feature.set('area_ha', feature.geometry().area().divide(10000)) #--> # Convert from square meters to hectares\n","\n","# Apply the function to each feature in the collection\n","area_mapped = gnut.map(calc_area)\n","\n","# Extract and print the area of each polygon\n","areas_info = area_mapped.getInfo()\n","\n","# Extracting areas into a list for plotting\n","areas = [feature['properties']['area_ha'] for feature in areas_info['features']]\n","\n","# Plotting the distribution of polygon areas\n","plt.figure(figsize=(10, 6))\n","plt.hist(areas, bins='auto', color='skyblue', alpha=0.8, rwidth=0.85)\n","plt.title('Distribution of Polygon Areas in Ha for Groundnut')\n","plt.xlabel('Area (Ha)')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2uPvoOP8Uujo"},"source":["Show area of each crop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvCoUdYvUPu_"},"outputs":[],"source":["#  'gnut', 'gsorrel', 'rice', etc. are already defined as ee.Geometry() or ee.Feature() objects representing each crop\n","crop_types = {\n","    'Gnut': gnut,\n","    'G. Sorrel': gsorrel,\n","    'Rice': rice,\n","    'Sesame': sesame,\n","    'Soye': soye,\n","    'Watermelon': melon,\n","    'Cassava': cassava,\n","    'Gnut Mix': gnutmix,\n","    'Millet Mix': milletmix,\n","    'Okra': okra,\n","    'Sorgh': sorgh,\n","    'Wheat': wheat,\n","    'Millet': millet,\n","    'Cowpea Mix': cowpmix,\n","    'Fallow': fallow\n","}\n","\n","# Calculate area for each crop and store in a dictionary\n","#areas = {name: crop.geometry().area().divide(10000).getInfo() for name, crop in crop_types.items()} #In Hectares\n","areas = {name: crop.geometry().area().getInfo() for name, crop in crop_types.items()}\n","\n","\n","# Create a DataFrame from the area dictionary\n","area_df = pd.DataFrame(list(areas.items()), columns=['Crop Type', 'Area (Squere meteres M2)'])\n","\n","# Display the DataFrame\n","print(area_df)\n","\n","area_df.head(14)"]},{"cell_type":"markdown","metadata":{"id":"jF3CRVX0V1wl"},"source":["Graphic of each crop in area coverages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbKXQ5YfVRtK"},"outputs":[],"source":["area_df = pd.DataFrame(list(areas.items()), columns=['Crop Type', 'Area (M2)'])\n","\n","# Sort the DataFrame by area for better visualization\n","area_df.sort_values('Area (M2)', ascending=False, inplace=True)\n","\n","# Plotting\n","plt.figure(figsize=(12, 8))\n","plt.bar(area_df['Crop Type'], area_df['Area (M2)'], color='red')\n","plt.xlabel('Crop Type')\n","plt.ylabel('Area in M2')\n","plt.title('Area of Each Crop Type for FY2023')\n","plt.xticks(rotation=90)  # Rotate crop type names for better readability\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lOcmfMnU0CuD"},"source":["### Count Pixels (NICFI) for each crop field represnetation of each specteral bands e.g Groundnut"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2KnVvBp0XoH"},"outputs":[],"source":["# time range\n","start_date = '2023-09-01'\n","end_date = '2023-09-30'\n","\n","# Load the Planet/NICFI imagery\n","imagery = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa').filterDate(start_date, end_date).filterBounds(merged2023)  # Assuming 'gnut' is the ee.Geometry() of my area of interest\n","#bands info\n","bands = ee.ImageCollection(imagery).first()\n","print(bands.bandNames().getInfo())\n","\n","#################  Planet Pixel count ####### Actual pixel count from PLANET\n","\n","# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Mask the image with the polygon\n","    masked_image = image.clip(gnut) #remember 1 polygone selected for closer see demo\n","\n","    # Count pixels - 4.77 X 4.77 m (~5X5m) actual Planet/NICFI pixel resolution\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=4.77,  # Scale in meters; 5m resolution take time to estimate (timeout)\n","        # to avoid time out estimation use 10X10m and convert it in to 5X5 by devide the result 4\n","        maxPixels=1e9  # Adjust maxPixels if needed to handle large areas\n","    )\n","\n","    # Need to return an ee.Feature for the .map() function\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzGYAjPY9tf0"},"outputs":[],"source":["# Define the time range\n","start_date = '2023-09-01'\n","end_date = '2023-09-30'\n","\n","\n","# Load the Planet/NICFI imagery\n","imagery = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa') \\\n","            .filterDate(start_date, end_date) \\\n","            .filterBounds(merged2023)\n","\n","# Print band information\n","bands = imagery.first()\n","print(bands.bandNames().getInfo())\n","\n","# Function to mask and count pixels within the AOI\n","def count_pixels(image):\n","    # Mask the image with the polygon\n","    masked_image = image.clip(merged2023)\n","\n","    # Count pixels - 10m resolution for faster computation\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=merged2023,\n","        scale=10,  # Scale in meters; coarser resolution for faster computation\n","        maxPixels=1e6  # Adjust maxPixels if needed to handle large areas\n","    )\n","\n","    # Need to return an ee.Feature for the .map() function\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"jKC_1ils9tNo"},"source":[]},{"cell_type":"markdown","metadata":{"id":"FJAcGlTUfoKE"},"source":["### pixel esimates for each crop considering area not pixels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaJnKXD9Z6l7"},"outputs":[],"source":["# Assuming these are the individual crop polygons. others to be added from other crop years.\n","crop_types = {\n","    'gnut': gnut, 'gsorrel': gsorrel, 'rice': rice, 'sesame': sesame,\n","    'soye': soye, 'watermelon': melon, 'cassava': cassava,\n","    'gnutmix': gnutmix, 'milletmix': milletmix, 'okra': okra,\n","    'sorgh': sorgh, 'wheat': wheat, 'millet': millet, 'cowpmix': cowpmix,\n","    'fallow': fallow\n","}\n","\n","def count_pixels(crop_name, crop_polygon):\n","    # Filter the imagery to the bounds of the crop polygon\n","    crop_imagery = imagery.filterBounds(crop_polygon)\n","\n","    # Apply the pixel counting for each image in the filtered collection\n","    def pixel_count(image):\n","        masked_image = image.clip(crop_polygon)\n","        pixel_count = masked_image.reduceRegion(\n","            reducer=ee.Reducer.count(),\n","            geometry=crop_polygon,\n","            scale=10,  # Adjust the scale based on the actual resolution of NICFI imagery\n","            maxPixels=1e9\n","        )\n","        return ee.Feature(None, pixel_count)\n","\n","    pixel_counts = crop_imagery.map(pixel_count)\n","    pixel_counts_info = pixel_counts.getInfo()\n","\n","    # Print results for each crop type\n","    print(crop_name)\n","    for feature in pixel_counts_info['features']:\n","        print(feature['properties'])\n","\n","# Run the pixel counting for each crop type\n","for crop_name, crop_polygon in crop_types.items():\n","    count_pixels(crop_name, crop_polygon)"]},{"cell_type":"markdown","metadata":{"id":"qXZyvvqShrCc"},"source":["### Pixel Exclusion representes less (e.g 40%) with in inside the polygon area.\n","\n","Masking Pixels: Pixels with less than 40% coverage by the polygon are excluded using the mask method, where mask = fraction.gte(0.4) creates a mask that includes only pixels where the fraction is 40% or more."]},{"cell_type":"markdown","metadata":{"id":"PEJuJdTiiXr8"},"source":["- Band Selection: When creating the mask, the code now ensures that a single band is selected using image.select(0). This selects the first band of the image, assuming the first band is suitable for your masking purposes. Adjust the band selection if necessary based on the bands available in your specific imagery.\n","\n","- Mask Application: By using single_band_image.mask() in the fraction calculation, we ensure that any operations involving masks deal with a single-band image, thus avoiding band mismatch errors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Mj5TnDQhkEx"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Ensure that we work with a single band for mask creation\n","    # Here, you could potentially select a specific band or reduce to a single band\n","    # As an example, if the image has multiple bands, use just one (e.g., the first band) for mask calculations\n","    single_band_image = image.select(1)\n","\n","    # Calculate the fraction of the pixel area that overlaps with the polygon using a constant image\n","    constantImage = ee.Image.constant(1).clip(okra)\n","    fraction = constantImage.updateMask(single_band_image.mask()).reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=gnut,\n","        scale=5,  # estimation with planet 5x5 is very difficult\n","        maxPixels=1e9\n","    ).get('constant')\n","\n","    # Threshold the fraction to include only pixels with >= 40% coverage\n","    mask = ee.Image(fraction).gte(0.4)\n","\n","    # Mask the image with the calculated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"9k0jFk_e9ChT"},"source":["only one polygon from gnut only for learning use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJhG_Dmb8zna"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Ensure that we work with a single band for mask creation\n","    single_band_image = image.select(1)  # Select the first band (assuming 1-based index)\n","\n","    # Calculate the fraction of the pixel area that overlaps with the polygon using a constant image\n","    constant_image = ee.Image.constant(1).clip(polygon1)\n","    fraction = constant_image.updateMask(single_band_image.mask()).reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=gnut.geometry(),\n","        scale=5,  # estimation with planet 5x5 is very difficult\n","        maxPixels=1e9\n","    ).get('constant')\n","\n","    # Threshold the fraction to include only pixels with >= 40% coverage\n","    mask = ee.Image.constant(fraction).gte(0.4)\n","\n","    # Mask the image with the calculated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut.geometry(),\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ILNKnZCXKUX"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Create a single band composite by averaging RGB and NIR bands\n","    composite = image.expression(\n","        \"(R + G + B + N) / 4\",  # Expression combining the bands\n","        {\n","            'R': image.select('R'),  #  'R' is the Red band\n","            'G': image.select('G'),  # 'G' is the Green band\n","            'B': image.select('B'),  #  'B' is the Blue band\n","            'N': image.select('N')   #  'N' is the NIR band\n","        }\n","    )\n","\n","    # Generate a mask based on the composite value, adjusting threshold as necessary\n","    mask = composite.gt(0.2)  # Example threshold\n","\n","    # Mask the image with the generated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"4A-gnC1OuL9G"},"source":["Pixel counts using area coverage regardless of the specteral bands (NICFI + 4.77m X 4.77 M resolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Bi2CGb9hUmU"},"outputs":[],"source":["resolution = 4.77  # Resolution of NICFI imagery in meters (4.77m x 4.77m per pixel)\n","\n","\n","# Function to calculate pixel count\n","def calculate_pixels(crop):\n","    area = crop.geometry().area()  # Area in square meters\n","    pixel_area = resolution * resolution  # Area of one pixel in square meters\n","    pixel_count = area.divide(pixel_area)  # Number of pixels\n","    return pixel_count.getInfo()\n","\n","# Calculate pixel count for each crop and store in a dictionary\n","pixel_counts = {name: calculate_pixels(crop) for name, crop in crop_types.items()}\n","\n","# Create a DataFrame from the area and pixel count dictionaries\n","area_df = pd.DataFrame(list(pixel_counts.items()), columns=['Crop Type', 'Pixel Count'])\n","\n","# Sort the DataFrame by 'Pixel Count' in descending order\n","area_df = area_df.sort_values(by='Pixel Count', ascending=False)\n","# Display the DataFrame\n","print(area_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jjx1cZ7akX9y"},"outputs":[],"source":["# Plotting\n","#plt.figure(figsize=(10, 6))\n","#plt.bar(area_df['Crop Type'], area_df['Pixel Count'], color='green')\n","#plt.xlabel('Crop Type')\n","#plt.ylabel('Pixel Count')\n","#plt.title('Pixel Count by Crop Type (Ascending Order)')\n","#plt.xticks(rotation=45, ha='right')\n","#plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","#plt.show()\n","####################\n","# Plotting using seaborn lib.\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Crop Type', y='Pixel Count', data=area_df, palette='viridis')  # Using 'viridis' palette for varying colors\n","plt.xlabel('Crop Type')\n","plt.ylabel('Pixel Count')\n","plt.title('Pixel Count by Crop Type - for data base 2(FY2023)')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VQ6KJlrreX7K"},"source":["Area vs Pixel count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mE924kWEev5z"},"outputs":[],"source":["# Assuming 'crop_types' dictionary is predefined with ee.Geometry() objects\n","resolution = 4.77  # Resolution of NICFI imagery in meters\n","\n","# Function to calculate pixel count\n","def calculate_pixels(area):\n","    pixel_area = resolution * resolution  # Area of one pixel in square meters\n","    return area / pixel_area\n","\n","# Calculate area and pixel count for each crop and store in dictionaries\n","area_and_pixels = []\n","for name, crop in crop_types.items():\n","    area = crop.geometry().area().getInfo()  # Area in square meters\n","    pixel_count = calculate_pixels(area)\n","    area_and_pixels.append((name, area, pixel_count))\n","\n","# Create a DataFrame from the results\n","df = pd.DataFrame(area_and_pixels, columns=['Crop Type', 'Area (Square Meters)', 'Pixel Count'])\n","\n","# Sort the DataFrame by 'Pixel Count' in descending order (optional)\n","df.sort_values(by='Pixel Count', ascending=False, inplace=True)\n","\n","# Plotting\n","fig, ax1 = plt.subplots(figsize=(14, 8))\n","\n","# Bar plot for Area using Seaborn\n","color = 'tab:red'\n","sns.barplot(x='Crop Type', y='Area (Square Meters)', data=df, palette='viridis', ax=ax1)\n","ax1.set_xlabel('Crop Type')\n","ax1.set_ylabel('Area (Square Meters)', color=color)\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","# Create a twin Axes sharing the x-axis for Pixel Count\n","ax2 = ax1.twinx()\n","color = 'tab:blue'\n","sns.lineplot(x='Crop Type', y='Pixel Count', data=df, sort=False, marker='o', color='red', ax=ax2)\n","ax2.set_ylabel('Pixel Count', color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","# Improve layout and set x-axis labels rotation\n","plt.xticks(rotation=90)\n","fig.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"istZFBD0pNjF"},"source":["### Garba\n","\n","Hi Garba, please use the above functions and generate the mean/median pixel specteral band values for \"R\", \"G\", \"B\" and \"N\" values of each pixel for each crop. Let me know if you need my assitance or have questions on the above functions. Perhaps you can show it in vector/tabular form.\n","\n","--------------------------------\n","R      |    G  |     B  |     N\n","-------------------------------\n","       |       |        |  "]},{"cell_type":"markdown","metadata":{"id":"wp0LBrEmCF9E"},"source":["**Steps**"]},{"cell_type":"markdown","metadata":{"id":"qPf6cPF1Sak1"},"source":["To enhance the quality and usability of the images for further analysis or applications Normalization and calibration of images are essential processes in image processing and computer vision.  "]},{"cell_type":"markdown","metadata":{"id":"J9_EfsreQFHk"},"source":["**1. Data Collection**: Obtain multispectral images of the crops that include the required spectral bands (R, G, B, and NIR)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9VAcmv-QGQy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vW0KWuhtQdRJ"},"source":["**2. Image Preprocessing**:"]},{"cell_type":"markdown","metadata":{"id":"BmSmJNVOSxOZ"},"source":["2.1. Image Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yypQbbs4S0pX"},"outputs":[],"source":["\n","\n","# Load the image\n","image = cv2.imread('/path/to/your/image.jpg')\n","\n","# Convert BGR image to RGB\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","# Normalize the image to range [0, 1]\n","normalized_image = image / 255.0\n","\n","# Alternatively, you can normalize to range [-1, 1]\n","# normalized_image = (image / 127.5) - 1.0\n","\n","# Display the original and normalized images\n","plt.subplot(1, 2, 1)\n","plt.title('Original Image')\n","plt.imshow(image)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Normalized Image')\n","plt.imshow(normalized_image)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"6bEH3KzTQnKp"},"source":["2.2. Calibration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxc9YkgeQtQS"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import glob\n","\n","# Termination criteria for corner sub-pixel accuracy\n","criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n","\n","# Prepare object points (0,0,0), (1,0,0), (2,0,0), ..., (6,5,0)\n","objp = np.zeros((6*7, 3), np.float32)\n","objp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)\n","\n","# Arrays to store object points and image points from all images\n","objpoints = []  # 3d points in real world space\n","imgpoints = []  # 2d points in image plane\n","\n","# Load calibration images\n","images = glob.glob('/path/to/calibration/images/*.jpg')\n","\n","for fname in images:\n","    img = cv2.imread(fname)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Find the chessboard corners\n","    ret, corners = cv2.findChessboardCorners(gray, (7, 6), None)\n","\n","    # If found, add object points, image points (after refining them)\n","    if ret:\n","        objpoints.append(objp)\n","        corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n","        imgpoints.append(corners2)\n","\n","        # Draw and display the corners\n","        img = cv2.drawChessboardCorners(img, (7, 6), corners2, ret)\n","        cv2.imshow('img', img)\n","        cv2.waitKey(500)\n","\n","cv2.destroyAllWindows()\n","\n","# Perform camera calibration\n","ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n","\n","# Save the calibration results\n","np.savez('calibration_data.npz', mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)\n","\n","# Undistort an image using the calibration results\n","img = cv2.imread('/path/to/your/test/image.jpg')\n","h, w = img.shape[:2]\n","newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n","\n","# Undistort\n","dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n","\n","# Crop the image\n","x, y, w, h = roi\n","dst = dst[y:y+h, x:x+w]\n","\n","# Display the result\n","cv2.imshow('calibrated', dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","metadata":{"id":"dHp5FV9LQwZu"},"source":["2.3. Alignment"]},{"cell_type":"markdown","metadata":{"id":"VXSHt0wTOjKa"},"source":["  **  Install OpenCV**: Install OpenCV in your Colab environment to use its functionalities.\n","\n","    **Load and Display Images**: Load the two images you want to align. Display them using Matplotlib to ensure they are loaded correctly.\n","\n","    **Detect ORB Features and Compute Descriptors**: Use the ORB detector to find keypoints and compute descriptors for both images.\n","\n","    **Match Features Using BFMatcher**: Match the descriptors using BFMatcher, and sort the matches based on their distance (quality).\n","\n","    **Find Homography and Warp Image**: Extract the coordinates of the matched points, compute the homography matrix using RANSAC, and apply the homography to warp the second image to align it with the first image."]},{"cell_type":"markdown","metadata":{"id":"goAziACvPDk3"},"source":["1. **Install OpenCV**: First, ensure you have OpenCV installed in your Colab environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHbd1sgXPHo7"},"outputs":[],"source":["!pip install opencv-python-headless\n"]},{"cell_type":"markdown","metadata":{"id":"oPIGL4ENPLSw"},"source":["**2. Load and Display Images**: Load the images you want to align."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOmwztSFPTVD"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the images in grayscale\n","img1 = cv2.imread('/path/to/your/image1.jpg', cv2.IMREAD_GRAYSCALE)\n","img2 = cv2.imread('/path/to/your/image2.jpg', cv2.IMREAD_GRAYSCALE)\n","\n","# Display the images\n","plt.subplot(1, 2, 1)\n","plt.title('Image 1')\n","plt.imshow(img1, cmap='gray')\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Image 2')\n","plt.imshow(img2, cmap='gray')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"e322vR2IPWSJ"},"source":["**3. Detect ORB Features and Compute Descriptor**s:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEY3EivzPcmu"},"outputs":[],"source":["# Initialize the ORB detector\n","orb = cv2.ORB_create()\n","\n","# Detect keypoints and compute descriptors\n","keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n","keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n"]},{"cell_type":"markdown","metadata":{"id":"MAFunTOGPf9o"},"source":["**4. Match Features Using BFMatcher**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjYdzM1BPo8B"},"outputs":[],"source":["# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(descriptors1, descriptors2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x: x.distance)\n","\n","# Draw top matches\n","img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","# Display the matches\n","plt.figure(figsize=(20, 10))\n","plt.imshow(img_matches)\n","plt.title('Feature Matches')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"g7L70895PwYz"},"source":["**5. Find Homography and Warp Image:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnjJP9wvOwpS"},"outputs":[],"source":["# Extract location of good matches\n","points1 = np.zeros((len(matches), 2), dtype=np.float32)\n","points2 = np.zeros((len(matches), 2), dtype=np.float32)\n","\n","for i, match in enumerate(matches):\n","    points1[i, :] = keypoints1[match.queryIdx].pt\n","    points2[i, :] = keypoints2[match.trainIdx].pt\n","\n","# Find homography matrix\n","h, mask = cv2.findHomography(points2, points1, cv2.RANSAC)\n","\n","# Use homography to warp the image\n","height, width = img1.shape\n","aligned_img = cv2.warpPerspective(img2, h, (width, height))\n","\n","# Display the aligned image\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1, 2, 1)\n","plt.title('Reference Image')\n","plt.imshow(img1, cmap='gray')\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Aligned Image')\n","plt.imshow(aligned_img, cmap='gray')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"dbVIGmM0MZ4a"},"source":["**Extract Pixel Values: Extract the pixel values for each spectral band (R, G, B, NIR) for each segmented crop area.**"]},{"cell_type":"markdown","metadata":{"id":"Y7pwVmGkQ2sU"},"source":["Clipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dqHNv9zQ6kL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"f9X91TDhQ63t"},"source":["3. **Segmentation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcx5V2x7RFcE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"j2b-hNkOR7H6"},"source":["**4. Extract Pixel Values**: Extract the pixel values for each spectral band (R, G, B, NIR) for each segmented crop area."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjFFEtWvR97w"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MS2G36cPSCTg"},"source":[]},{"cell_type":"markdown","metadata":{"id":"JVd_6vKWMcwM"},"source":["**5. Compute Statistics:**\n","\n","    Mean: Calculate the mean value for each spectral band across all pixels for each crop.\n","    Median: Calculate the median value for each spectral band across all pixels for each crop."]},{"cell_type":"markdown","metadata":{"id":"paLLSEy7GqzS"},"source":["### Cyrille\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwxxInMsG5Aa"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"myjnM5EYfOBA"},"source":["**GARBA**  Other Proposition"]},{"cell_type":"markdown","metadata":{"id":"H1koAg51fR0v"},"source":[]},{"cell_type":"markdown","metadata":{"id":"U13_ffz19_4S"},"source":["[**Click Here for the first step to follow**](https://code.earthengine.google.com/91904df41cc77089f346977c173a0122)"]},{"cell_type":"markdown","metadata":{"id":"_8Y9N4Ny_H3T"},"source":["Change the variable to our variable\n","\n","1.   Area of Interest(roi)\n","2.   Land cover entities\n","3.   Filter the date\n","4.   Setup the other parameters for the convenance  \n"]},{"cell_type":"markdown","metadata":{"id":"jvaGAoM3fbL-"},"source":["Install some packages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMIrI88_fpF7"},"outputs":[],"source":["!pip install rasterio\n","!pip install earthpy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxwz3-lzgUvO"},"outputs":[],"source":["#Import packages\n","import pandas as pd\n","import numpy as np\n","import keras\n","from keras import sequential\n","from keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Input, GlobalMawPooling1D\n","from keras.callbacks import EarlyStopping\n","from kera import Model\n","import rasterio\n","import earth.plot as ep\n","from keras.utils import to_categorical\n","import sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, Classification_report\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import from_levels_and_colors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHxdqr3Vgpla"},"outputs":[],"source":["#Mount GDRIVE\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"wNOesrFqjnXm"},"source":[]},{"cell_type":"markdown","metadata":{"id":"zk_oCPdRjWcy"},"source":["Drive already mounted at/content/drive; to attemp to forcibly remount, call drive.mount(\"/content/drive\" force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"Xp37g2MLi6h4"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOO4LRyLkY60"},"outputs":[],"source":["# Parameter\n","FEATURES = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'EVI', 'NBR', 'NDMI', 'NDWI', 'NDBI', 'NDBaI', 'elevation']\n","LABEL = ['classvalue']\n","SPLIT = ['sample']\n","N_CLASSES = 9\n","CLASSES = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n","PALETTE = ['#F08080', '#D2B48C', '#87CEFA', '#008080', '#90EE90', '#228B22', '#808000', '#FF8C00', '#006400']\n","SAMPLE_PATH = '/content/drive/MyDrive/DL/Samples_LC_Jambi_2023.csv'\n","IMAGE_PATH = '/content/drive/MyDrive/DL/Landsat_Jambi_2023.tif'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpSnMGQ_7UJe"},"outputs":[],"source":["# Load image\n","image = rasterio.open(IMAGE_PATH)\n","bandNum = image.count\n","height = image.height\n","width = image.width\n","crs = image.crs\n","transform = image.transform\n","shape = (height, width)\n","\n","image_vis = []\n","for x in [6, 5, 4]:\n","  image_vis.append(image.read(x))\n","image_vis = np.stack(image_vis)\n","\n","plot_size = (8, 8)\n","ep.plot_rgb(\n","  image_vis,\n","  figsize=plot_size,\n","  stretch=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNeYJkf47Vjc"},"outputs":[],"source":["# Read sample\n","samples = pd.read_csv(SAMPLE_PATH)\n","samples = samples.sample(frac = 1) # Shuffle data\n","samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hetXu1KB7lRR"},"outputs":[],"source":["# Split into train and test based on column\n","train = samples[samples['sample'] == 'train']\n","test = samples[samples['sample'] == 'test']\n","\n","# Split between features and label\n","train_features = train[FEATURES]\n","train_label = train[LABEL]\n","test_features = test[FEATURES]\n","test_label = test[LABEL]\n","\n","# Function to reshape array input\n","def reshape_input(array):\n","  shape = array.shape\n","  return array.reshape(shape[0], shape[1], 1)\n","\n","# Convert samples dataframe (pandas) to numpy array\n","train_input = reshape_input(train_features.to_numpy())\n","test_input = reshape_input(test_features.to_numpy())\n","\n","# Also make label data to categorical\n","train_output = to_categorical(train_label.to_numpy(), N_CLASSES + 1, int)\n","test_output = to_categorical(test_label.to_numpy(), N_CLASSES + 1, int)\n","\n","# Show the data shape\n","print(f'Train features: {train_input.shape}\\nTest features: {test_input.shape}\\nTrain label: {train_output.shape}\\nTest label: {test_output.shape}')"]},{"cell_type":"markdown","metadata":{"id":"VbTZopVG7y6L"},"source":["Train features: (14853, 14, 1)\n","Test features: (4043, 14, 1)\n","Train label: (14853, 10)\n","Test label: (4043, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eg5xlKwj70hr"},"outputs":[],"source":["# Make model for our data\n","# Input shape\n","train_shape = train_input.shape\n","input_shape = (train_shape[1], train_shape[2])\n","\n","# Model parameter\n","neuron = 64\n","drop = 0.2\n","kernel = 2\n","pool = 2\n","\n","# Make sequential model\n","model = Sequential([\n","  Input(input_shape),\n","  Conv1D(neuron * 1, kernel, activation='relu'),\n","  Conv1D(neuron * 1, kernel, activation='relu'),\n","  MaxPooling1D(pool),\n","  Dropout(drop),\n","  Conv1D(neuron * 2, kernel, activation='relu'),\n","  Conv1D(neuron * 2, kernel, activation='relu'),\n","  MaxPooling1D(pool),\n","  Dropout(drop),\n","  GlobalMaxPooling1D(),\n","  Dense(neuron * 2, activation='relu'),\n","  Dropout(drop),\n","  Dense(neuron * 1, activation='relu'),\n","  Dropout(drop),\n","  Dense(N_CLASSES + 1, activation='softmax')\n","])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LrRmOi-8HbG"},"outputs":[],"source":["# Train the model\n","\n","# Compline the model\n","model.compile(\n","    optimizer='Adam',\n","    loss='CategoricalCrossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Create callback to stop training if loss not decreasing\n","stop = EarlyStopping(\n","    monitor='loss',\n","    patience=5\n",")\n","\n","# Fit the model\n","result = model.fit(\n","    x=train_input, y=train_output,\n","    validation_data=(test_input, test_output),\n","    batch_size=1024,\n","    callbacks=[stop],\n","    epochs=100,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQjK3ddi79xf"},"outputs":[],"source":["# Show history\n","history = pd.DataFrame(result.history)\n","\n","plt.figure(figsize = (10, 8))\n","plt.plot(range(len(history['accuracy'].values.tolist())), history['accuracy'].values.tolist(), label = 'Train_Accuracy')\n","plt.plot(range(len(history['loss'].values.tolist())), history['loss'].values.tolist(), label = 'Train_Loss')\n","plt.plot(range(len(history['val_accuracy'].values.tolist())), history['val_accuracy'].values.tolist(), label = 'Test_Accuracy')\n","plt.plot(range(len(history['val_loss'].values.tolist())), history['val_loss'].values.tolist(), label = 'Test_Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Value')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDJoUR758jEU"},"outputs":[],"source":["# Predict test data\n","prediction = np.argmax(model.predict(test_input), 1).flatten()\n","label = np.argmax(test_output, 1).flatten()\n","\n","# Confusion matrix\n","cm = confusion_matrix(label, prediction, normalize='true')\n","cm = ConfusionMatrixDisplay(cm)\n","cm.plot()\n","\n","# Classification report\n","print(classification_report(label, prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_I6kCV2i8kH0"},"outputs":[],"source":["# Predict image using the model\n","image_input = []\n","for x in range(14):\n","  image_input.append(image.read(x + 1))\n","image_input = reshape_input(np.stack(image_input).reshape(14, -1).T)\n","\n","# Predict\n","prediction = model.predict(image_input, batch_size=4096*20)\n","prediction = np.argmax(prediction, 1)\n","prediction = prediction.reshape(shape[0], shape[1])\n","\n","# Visualize\n","cmap, norm = from_levels_and_colors(CLASSES, PALETTE, extend='max')\n","ep.plot_bands(prediction, cmap=cmap, norm=norm, figsize=plot_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OIrjTHJ8s7I"},"outputs":[],"source":["# Save file to drive\n","save_location = '/content/drive/MyDrive/DL/'\n","name = 'LC_Jambi_2023.tif'\n","location = save_location + name\n","\n","new_dataset = rasterio.open(\n","      location,\n","      mode='w', driver='GTiff',\n","      height = prediction.shape[0], width = prediction.shape[1],\n","      count=1, dtype=str(prediction.dtype),\n","      crs=crs,\n","      transform=transform\n",")\n","new_dataset.write(prediction, 1);\n","new_dataset.close()"]},{"cell_type":"markdown","metadata":{"id":"4bilAgHAAMg4"},"source":["For following step by step [Youtube Chanel](https://www.youtube.com/watch?v=NFoZPyQqVRA) **source**: Ramadhan"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
